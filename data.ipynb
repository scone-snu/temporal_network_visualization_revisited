{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-28T14:26:52.998577",
     "start_time": "2017-03-28T14:26:52.994106"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import subprocess\n",
    "import re\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-28T14:01:36.749710",
     "start_time": "2017-03-28T14:01:36.746391"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_binned_network(filename):\n",
    "    with open('data_source/' + filename +'.pkl', 'r') as infile:\n",
    "        return pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-28T14:55:12.630491",
     "start_time": "2017-03-28T14:55:12.356567"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Infomap(pajek_string, *args, **kwargs):\n",
    "    \"\"\"Function that pipes commands to subprocess and runs native Infomap implementation.\n",
    "    \n",
    "    Requires two folders (1) 'input' and (2) 'output', in sister-directory of 'infomap' folder\n",
    "    that contains 'Infomap' executable. To setup 'infomap' folder, close Infomap from \n",
    "    https://github.com/mapequation/infomap and run 'make' inside resulting folder.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pajek_string : str\n",
    "        Pajek representation of the network (str)\n",
    "    *args : dict\n",
    "        Infomap execution options. (http://www.mapequation.org/code.html#Options)\n",
    "    Returns\n",
    "    -------\n",
    "    communities : list of lists\n",
    "    layer_communities : data structure in required format for d3 viz (json)\n",
    "    \"\"\"\n",
    "    \n",
    "    def _get_id_to_label(filename):\n",
    "        def __int_if_int(val):\n",
    "            try: return int(val)\n",
    "            except ValueError: return val\n",
    "        with open('community_detection/input/' + filename + \".net\", 'r') as fp:\n",
    "            parsed_network = fp.read()\n",
    "        return dict(\n",
    "            (int(n.split()[0]), __int_if_int(n.split('\"')[1]))\n",
    "            for n in re.split(r\"\\*.+\", parsed_network)[1].split(\"\\n\")[1:-1]\n",
    "        )\n",
    "    \n",
    "    def _parse_communities_multiplex(id_to_label, filename):\n",
    "        with open('community_detection/output/'+filename+\"_expanded.clu\", 'r') as infile:\n",
    "            clusters = infile.read()\n",
    "\n",
    "        # Get layers, nodes and clusters from _extended.clu file\n",
    "        la_no_clu_flow = re.findall(r'\\d+ \\d+ \\d+ \\d\\.\\d+', clusters) # [\"30 1 2 0.00800543\",...]\n",
    "        la_no_clu_flow = [tuple(i.split()) for i in la_no_clu_flow]\n",
    "\n",
    "        node_flow_json = defaultdict(float)      # {layer_node: flow, ...}\n",
    "        community_flow_json = defaultdict(float) # {community: flow, ...}\n",
    "        communities_json = defaultdict(set)      # {layer: {(node, cluster), ...}, ...}\n",
    "        for layer, node, cluster, flow in la_no_clu_flow:\n",
    "            node_flow_json[(int(layer), id_to_label[int(node)])] += float(flow)\n",
    "            community_flow_json[cluster] += float(flow)\n",
    "            communities_json[int(layer)].add((id_to_label[int(node)], int(cluster)))\n",
    "\n",
    "        return communities_json, node_flow_json, community_flow_json\n",
    "    \n",
    "    def _parse_communities_planar(id_to_label, filename):\n",
    "        with open('community_detection/output/'+filename+\".clu\", 'r') as infile:\n",
    "            clusters = infile.read()\n",
    "        \n",
    "        # Get nodes and clusters from .clu file\n",
    "        no_clu = [tuple(i.split()[:-1]) for i in re.findall(r\"\\d+ \\d+ \\d\\.\\d+\", clusters)]  # [(node, cluster), ...]\n",
    "        return {0: set([(id_to_label[int(no)], int(clu)) for no, clu in no_clu])}\n",
    "    \n",
    "    def _clean_up(filename):\n",
    "        subprocess.call(['rm', 'community_detection/input/'+filename+'.net'])\n",
    "        subprocess.call(['rm', 'community_detection/output/'+filename+'_expanded.clu'])\n",
    "        subprocess.call(['rm', 'community_detection/output/'+filename+'.clu'])\n",
    "    \n",
    "    # Check for process id in args (for multiprocessing)\n",
    "    if args[-1][:3] == \"pid\":\n",
    "        pid = args[-1][3:]\n",
    "        args = args[:-1]\n",
    "    else:\n",
    "        pid = \"\"\n",
    "        \n",
    "    # Set default kwarg params\n",
    "    return_flow = kwargs.get(\"return_flow\", False)\n",
    "        \n",
    "    # Get network in multiplex string format and define filename\n",
    "    filename = 'tmpnet' + pid\n",
    "\n",
    "    # Store locally\n",
    "    with open(\"community_detection/input/\"+filename+\".net\", 'w') as outfile:\n",
    "        outfile.write(pajek_string)\n",
    "    \n",
    "    # Run Infomap for multiplex network\n",
    "    subprocess.call(\n",
    "        ['./community_detection/infomap/Infomap', 'community_detection/input/'+filename+\".net\"] + \\\n",
    "        list(args)\n",
    "    )\n",
    "    \n",
    "    # Parse communities from Infomap output\n",
    "    id_to_label = _get_id_to_label(filename)\n",
    "    \n",
    "    if 'multiplex' in list(args):\n",
    "        parsed_communities, node_flow, community_flow = _parse_communities_multiplex(id_to_label, filename)\n",
    "    if 'pajek' in list(args):\n",
    "        parsed_communities = _parse_communities_planar(id_to_label, filename)\n",
    "        \n",
    "    _clean_up(filename)\n",
    "\n",
    "    orig_clu = [item for sublist in parsed_communities.values() for item in sublist]\n",
    "    communities = dict()\n",
    "    for key, group in itertools.groupby(orig_clu, lambda x: x[1]):\n",
    "        for thing in group:\n",
    "            try:\n",
    "                communities[key].append(thing[0])\n",
    "            except KeyError:\n",
    "                communities[thing[1]] = [thing[0]]\n",
    "    communities = dict((k,set(v)) for k,v in communities.items())\n",
    "\n",
    "    layer_communities = {}\n",
    "    for layer, group in parsed_communities.items():\n",
    "        communities = {}\n",
    "        for no, clu in group:\n",
    "            try:\n",
    "                communities[clu-1].append(no)\n",
    "            except KeyError:\n",
    "                communities[clu-1] = [no]\n",
    "        layer_communities[layer] = communities\n",
    "\n",
    "    if return_flow:\n",
    "        return communities, layer_communities, node_flow, community_flow\n",
    "    else:\n",
    "        return communities, layer_communities\n",
    "    \n",
    "    \n",
    "def build_adjacency_tensor(layers, index=\"zero\"):\n",
    "    nodes = set([\n",
    "        n\n",
    "        for l in layers\n",
    "        for n in list(l['user1']) + list(l['user2'])\n",
    "    ])\n",
    "    \n",
    "    ind = dict((n, i) for i, n in enumerate(nodes))\n",
    "    \n",
    "    A = defaultdict(int)\n",
    "    for l, layer in enumerate(layers):\n",
    "        for _, row in layer.iterrows():\n",
    "            # Must add both ways if undirected so A becomes symmetrical. If only added one-way\n",
    "            # triu will only be connections from 'user' and and tril from 'bt_mac' or vice versa.\n",
    "            if index == \"zero\":\n",
    "                A[(ind[row['user1']], ind[row['user2']], l)] += 1\n",
    "                A[(ind[row['user2']], ind[row['user1']], l)] += 1\n",
    "            else:\n",
    "                A[(row['user1'], row['user2'], l)] += 1\n",
    "                A[(row['user2'], row['user1'], l)] += 1\n",
    "    return A\n",
    "\n",
    "\n",
    "def write_pajek(A, node_labels=None, index_from=0):\n",
    "    \"\"\"Return multiplex representation of multiplex network adjacency matrix A\n",
    "    \n",
    "    Providing an adjacency tensor where A[:, :, k] is adjacency matrix of temporal\n",
    "    layer k, return a pajek format representation of the temporal network which weights interlayer\n",
    "    edges by state node neighborhood similarity. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A : numpy.3darray\n",
    "        3d tensor where each A[:, :, k] is a layer adjacency matrix\n",
    "    node_labels : list\n",
    "        List of node labels if (optional)\n",
    "    index_from : int\n",
    "        From which number to index nodes and layers in pajek format from (default=0)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out : string\n",
    "        A network string in pajek format\n",
    "    \"\"\"\n",
    "    \n",
    "    def _write_outfile(A):\n",
    "        \"\"\"Write nodes and intra/inter-edges from A and J to string.\"\"\"\n",
    "        def __remove_symmetry_A(A):\n",
    "            A_triu = defaultdict(int)\n",
    "            for (i, j, k), w in A.items():\n",
    "                if j > i:\n",
    "                    A_triu[(i, j, k)] = w\n",
    "            return A_triu\n",
    "        def __write_nodes(outfile):\n",
    "            outfile += \"*Vertices %d\" % Nn\n",
    "            for nid, label in enumerate(nodes):\n",
    "                outfile += '\\n%d \"%s\" 1.0' % (nid + index_from, str(label))\n",
    "            return outfile\n",
    "        def __write_intra_edges(outfile):\n",
    "            outfile += \"\\n*Intra\\n# layer node node [weight]\"\n",
    "            for (i, j, k), w in __remove_symmetry_A(A).items():\n",
    "                outfile += '\\n%d %d %d %f' % (\n",
    "                    k + index_from,  # layer\n",
    "                    nodemap[i] + index_from,  # node\n",
    "                    nodemap[j] + index_from,  # node\n",
    "                    w                # weight\n",
    "                )\n",
    "            return outfile\n",
    "        \n",
    "        outfile = \"\"\n",
    "        outfile = __write_nodes(outfile)\n",
    "        outfile = __write_intra_edges(outfile)\n",
    "        \n",
    "        return outfile\n",
    "    \n",
    "    nodes = sorted(set([n for i, j, _ in A.keys() for n in [i, j]]))\n",
    "    Nn = len(nodes)\n",
    "    Nl = len(set([k for i, j, k in A.keys()]))\n",
    "    \n",
    "    nodemap = dict(zip(nodes, range(Nn)))\n",
    "\n",
    "    return _write_outfile(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-28T14:01:40.724918",
     "start_time": "2017-03-28T14:01:37.175589"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network_sensible = load_binned_network('10mins_short'); fof = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-28T14:51:59.515745",
     "start_time": "2017-03-28T14:51:50.449388"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make slices for a span of days (e.g. monday to friday)\n",
    "# The below configuration gives exactly the three-week period in january 2014\n",
    "spd = 288 / fof  # slices per day\n",
    "smargin_start = 0#spd / 3\n",
    "smargin_end = 0#spd / 3 - 6\n",
    "dow = 0\n",
    "\n",
    "network = [\n",
    "    l\n",
    "    for d in [0, 1]\n",
    "    for l in network_sensible[spd*(dow+5+d)+smargin_start:spd*(dow+6+d)-smargin_end]\n",
    "]\n",
    "\n",
    "layer_indices = [l for l, n in enumerate(network) if n.shape[0] > 0]\n",
    "A = build_adjacency_tensor([n for n in network if n.shape[0] > 0], index=None)\n",
    "network_pajek = write_pajek(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-28T14:55:44.353025",
     "start_time": "2017-03-28T14:55:16.745726"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_, layer_commu = Infomap(\n",
    "        network_pajek,\n",
    "        'community_detection/output/',\n",
    "        '-i',\n",
    "        'multiplex',\n",
    "        '--multiplex-js-relax-rate', '0.25',\n",
    "        '--overlapping',\n",
    "        '--expanded',\n",
    "        '--clu',\n",
    "        '--two-level',\n",
    "        '-z',\n",
    "        'pid%d' % random.randint(0, 1000000)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-28T14:55:53.456700",
     "start_time": "2017-03-28T14:55:53.447311"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_nodes = set([\n",
    "    n\n",
    "    for l in layer_commu.keys()\n",
    "    for nodes in layer_commu[l].values()\n",
    "    for n in nodes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-28T14:55:55.180680",
     "start_time": "2017-03-28T14:55:53.957592"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds_out = []\n",
    "for nn in all_nodes:\n",
    "    for l, data in layer_commu.items():\n",
    "        for c, nodes in data.items():\n",
    "            if nn in nodes:\n",
    "                for n in nodes:\n",
    "                    ds_out.append(nn)\n",
    "                    \n",
    "Counter(ds_out).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-28T21:42:39.163064",
     "start_time": "2017-03-28T21:42:39.142124"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "858\n"
     ]
    }
   ],
   "source": [
    "ds_out = []\n",
    "for l, data in layer_commu.items():\n",
    "    for c, nodes in data.items():\n",
    "        if len(set([300]) & set(nodes)) > 0:\n",
    "            for n in nodes:\n",
    "                ds_out.append([n, l, c])\n",
    "            \n",
    "print len(ds_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-28T21:42:39.755419",
     "start_time": "2017-03-28T21:42:39.748930"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(ds_out, columns=['id', 'timestamp', 'group']) \\\n",
    "    .to_csv(\"data/ds_300_2_days.tsv\", \"\\t\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infection starting at node 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-28T21:42:43.850653",
     "start_time": "2017-03-28T21:42:43.822664"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17231\n"
     ]
    }
   ],
   "source": [
    "ds_out = []\n",
    "infected = set([300])\n",
    "for l, data in sorted(layer_commu.items(), key=lambda (k, v): k):\n",
    "    for c, nodes in data.items():\n",
    "        if len(infected & set(nodes)) > 0:\n",
    "            for n in nodes:\n",
    "                ds_out.append([n, l, c])\n",
    "                infected.add(n)\n",
    "            \n",
    "print len(ds_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-28T21:42:52.008245",
     "start_time": "2017-03-28T21:42:51.973374"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(ds_out, columns=['id', 'timestamp', 'group']) \\\n",
    "    .to_csv(\"data/ds_300_infection_2_days.tsv\", \"\\t\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-28T21:43:19.079842",
     "start_time": "2017-03-28T21:43:18.932502"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34148\n"
     ]
    }
   ],
   "source": [
    "ds_out = []\n",
    "infected = set([300])\n",
    "for l, data in sorted(layer_commu.items(), key=lambda (k, v): k):\n",
    "    for c, nodes in data.items():\n",
    "        for n in nodes:\n",
    "            ds_out.append([n, l, c])\n",
    "            infected.add(n)\n",
    "            \n",
    "print len(ds_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-28T21:43:20.522306",
     "start_time": "2017-03-28T21:43:20.459510"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(ds_out, columns=['id', 'timestamp', 'group']) \\\n",
    "    .to_csv(\"data/ds_full_2_days.tsv\", \"\\t\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full network (minimum community size 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-28T21:43:59.226249",
     "start_time": "2017-03-28T21:43:59.200996"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18224\n"
     ]
    }
   ],
   "source": [
    "ds_out = []\n",
    "infected = set([300])\n",
    "for l, data in sorted(layer_commu.items(), key=lambda (k, v): k):\n",
    "    for c, nodes in data.items():\n",
    "        if len(nodes) > 3:\n",
    "            for n in nodes:\n",
    "                ds_out.append([n, l, c])\n",
    "                infected.add(n)\n",
    "            \n",
    "print len(ds_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-28T21:44:08.297333",
     "start_time": "2017-03-28T21:44:08.266649"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(ds_out, columns=['id', 'timestamp', 'group']) \\\n",
    "    .to_csv(\"data/ds_full_min_size_4_2_days.tsv\", \"\\t\", header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
